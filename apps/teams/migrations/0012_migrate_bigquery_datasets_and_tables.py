# Generated by Django 3.2.5 on 2021-08-21 23:32

from functools import cache

from apps.base.clients import get_credentials
from django.conf import settings
from django.db import migrations
from django.utils.text import slugify
from google.cloud import bigquery


@cache
def bigquery_client():
    # https://cloud.google.com/bigquery/external-data-drive#python
    credentials, project = get_credentials()

    # return bigquery.Client(project=settings.GCP_PROJECT)
    return bigquery.Client(
        credentials=credentials, project=project, location=settings.BIGQUERY_LOCATION
    )


def _migrate_team(team, Table, DATASET_ID, DATAFLOW_ID):

    client = bigquery_client()

    NEW_SLUG = (
        slugify(settings.CLOUD_NAMESPACE)
        if settings.CLOUD_NAMESPACE is not None
        else None
    )
    dataset_id = "team_{self.id:06}_tables"
    dataset_id = f"{NEW_SLUG}_{dataset_id}" if NEW_SLUG is not None else dataset_id

    # create the dataset in bigquery
    # ignore errors if already exists
    client.create_dataset(dataset_id, exists_ok=True)

    # create copies of all relevant tables

    # integrations
    for table in Table.objects.filter(
        integration__project__team=team, integration__kind__in=["upload", "sheet"]
    ).all():
        # update the dataset and id information
        new_table_id = (
            f"upload_{table.integration.upload.id}"
            if table.integration.kind == "upload"
            else f"sheet_{table.integration.sheet.id}"
        )

        # migrate the table
        client.copy_table(
            f"{DATASET_ID}.table_{table.id}", f"{dataset_id}.{new_table_id}"
        )

        table._bq_table = new_table_id
        table.bq_dataset = dataset_id
        table.save()

    for table in Table.objects.filter(
        workflow_node__workflow__project__team=team
    ).all():
        # update the dataset and id information
        new_table_id = table.workflow_node.bq_output_table_id

        # migrate the table
        client.copy_table(
            f"{DATAFLOW_ID}.{table._bq_table}", f"{dataset_id}.{new_table_id}"
        )

        table._bq_table = new_table_id
        table.bq_dataset = dataset_id
        table.save()

    # we do not need to migrate intermediate tables, they can be recreated

    # unfortunately, fivetran schemas are immutable


def forwards_func(apps, schema_editor):

    print("Starting bigquery migration...")

    # "heroku" is for production, in the new system we've removed the slug
    OLD_SLUG = slugify(settings.CLOUD_NAMESPACE or "heroku")

    DATASET_ID = f"{OLD_SLUG}_integrations"
    DATAFLOW_ID = f"{OLD_SLUG}_dataflows"

    # migrate all bigquery datasets and tables
    # we will make a copy, so everything is backwards compatible

    Team = apps.get_model("teams", "Team")
    Table = apps.get_model("tables", "Table")

    for team in Team.objects.all():
        print(f"...migrating team {team.id}...")
        _migrate_team(team, Table, DATASET_ID, DATAFLOW_ID)

    print("...done.")


class Migration(migrations.Migration):

    dependencies = [
        ("teams", "0011_remove_team_enabled"),
    ]

    operations = [
        migrations.RunPython(forwards_func, reverse_code=migrations.RunPython.noop)
    ]
