# Generated by Django 3.2.5 on 2021-08-21 23:32

from functools import cache

from apps.base.clients import get_credentials
from django.conf import settings
from django.db import migrations
from django.utils.text import slugify
from google.api_core.exceptions import NotFound
from google.cloud import bigquery


@cache
def bigquery():
    # https://cloud.google.com/bigquery/external-data-drive#python
    credentials, project = get_credentials()

    # return bigquery.Client(project=settings.GCP_PROJECT)
    return bigquery.Client(
        credentials=credentials, project=project, location=settings.BIGQUERY_LOCATION
    )


def _migrate_team(team, Table, DATASET_ID, DATAFLOW_ID):

    client = bigquery()

    NEW_SLUG = (
        slugify(settings.CLOUD_NAMESPACE)
        if settings.CLOUD_NAMESPACE is not None
        else None
    )
    dataset_id = f"team_{team.id:06}_tables"
    dataset_id = f"{NEW_SLUG}_{dataset_id}" if NEW_SLUG is not None else dataset_id

    # create the dataset in bigquery
    # ignore errors if already exists
    client.create_dataset(dataset_id, exists_ok=True)

    # create copies of all relevant tables

    # integrations
    for table in Table.objects.filter(
        integration__project__team=team, integration__kind__in=["upload", "sheet"]
    ).all():
        # update the dataset and id information
        new_table_id = (
            f"upload_{table.integration.upload.id:09}"
            if table.integration.kind == "upload"
            else f"sheet_{table.integration.sheet.id:09}"
        )

        # migrate the table
        try:
            client.copy_table(
                f"{DATASET_ID}.table_{table.id}", f"{dataset_id}.{new_table_id}"
            )
        except NotFound:
            pass

        table._bq_table = new_table_id
        table.bq_dataset = dataset_id
        table.save()

    for table in Table.objects.filter(
        workflow_node__workflow__project__team=team
    ).all():
        # update the dataset and id information
        new_table_id = f"output_node_{table.workflow_node.id:09}"

        # migrate the table
        try:
            client.copy_table(
                f"{DATAFLOW_ID}.table_{table.workflow_node.id}",
                f"{dataset_id}.{new_table_id}",
            )
        except NotFound:
            pass

        table._bq_table = new_table_id
        table.bq_dataset = dataset_id
        table.save()

    # we do not need to migrate intermediate tables, they can be recreated

    # unfortunately, fivetran schemas are immutable


def forwards_func(apps, schema_editor):

    print("Starting bigquery migration...")

    # mapping from new to old cloud namespace
    # for prod, None denotes that the namespace is not used at all
    cloud_namespace_map = {"mvp": "heroku", "release": "heroku_release", None: "heroku"}

    cloud_namespace = cloud_namespace_map.get(
        settings.CLOUD_NAMESPACE, settings.CLOUD_NAMESPACE
    )
    OLD_SLUG = slugify(cloud_namespace)

    DATASET_ID = f"{OLD_SLUG}_integrations"
    DATAFLOW_ID = f"{OLD_SLUG}_dataflows"

    # migrate all bigquery datasets and tables
    # we will make a copy, so everything is backwards compatible

    Team = apps.get_model("teams", "Team")
    Table = apps.get_model("tables", "Table")

    for team in Team.objects.all():
        print(f"...migrating team {team.id}...")
        _migrate_team(team, Table, DATASET_ID, DATAFLOW_ID)

    print("...done.")


class Migration(migrations.Migration):

    dependencies = [
        ("tables", "0011_alter_table_options"),
    ]

    operations = [
        migrations.RunPython(forwards_func, reverse_code=migrations.RunPython.noop)
    ]
